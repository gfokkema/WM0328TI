# Drones and Responsibility: mapping the field
**Filippo Santoni de Sio & Ezio Di Nucci**

## Introduction

The academic ethical question has been posed, whether it is alright to use remotely controlled
weapons in (lethal) military operations. One more specific issue has been discussed: whether the
use of machines that enjoy some leve of autonomy in military operations may lead to ethically
unacceptable `responsibility gaps' -- circumstances in which an untoward event occurs and no
human agent can legitimately be held responsible for it.

The present volume aims to analyze in more depth the relationship between the use of drones in
military operation and various issues of responsibility. It has something in common with both
of the existing debates, but also significantly exceeds them in scope.

## State of the art: responsibility and the *responsibility gap*
The responsility gap refers to the idea that, when technology which has a certain degree of 
autonomy is deployed, there is a risk that no human will be responsible for what this autonomous
or semi-autonomous technology will do. For example in case such technology should happen to
malfunction and to create negative unwanted effects. This responsibility gap is then taken to be
an ethical reason against deploying (semi-) autonomous technology.

Sparrow's argument is that there is no plausible candidate for the bearer of responsibility (say
for war crimes): the programmer is not a plausible candidate, the commanding officer is not a 
plausible candidate and the machine itself is also not a plausible candidate. *"If we insist that
the officer is responsible, this is at the cost of allowing that they should sometimes be held 
entirely responsible for actions over which they had no control."*

We agree with Sparrow that the machine itself cannot be the bearer of responsibility. We also agree
with Sparrow that there will be at least some cases where the programmers cannot be made legitimately
respponsible for what has gone wrong, either because they had actually mentioned the risk to the
decision makers or because the failing could not reasonably have been predicted by programmers.

Sparrow's argument is at its weakest concerning the decision maker (Sparrow's Commanding Officer):
he argues that the commanding officer cannot be held responsible for everything the machines will
do because *"the autonomy of the machine implies that its orders to not determine (although they
obviously influence) its actions"*.

This principle - understood as a necessary condition on responsibility according to which a 
commanding officer or decision maker can only be responsile if its orders determine the relevant
actions which will be carried out - is implausible. Firstly because whether we live in a deterministic
or indeterministic world, orders will never determine actions whether of humans or robots. So the 
kind of determination meant cannot be that of the thesis of causal determinism, but even taking the
principle to rather mean something about orders being a proximate cause in a reliable chain leading
to action, it would remain implausible because it would negate responsibility in most cases of ignorance,
negligence and bad luck. Indeed, Sparrow has in mind a particular sense of “moral responsibility” as
relying on a strong control condition which, as philosophically defensible as it might be, is certainly
not the (only) one used in common morality, let alone in the law. So Sparrow’s principle is, as a general
necessary condition for responsibility, way too weak.

Moreover, as far as military robots are concerned, that the commanding officer will not be
responsible for everything that the machine does is not a problem, because one may clearly also
accept a pluralist view according to which sometimes the programmer is responsible. However,
Sparrow’s claim is that sometimes the commanding officer is not responsible and no one else is
and that is the problem.

But even if one accepts that the machine’s orders do not fully determine its actions, that in turn
does not imply that sometimes the commanding officer will be - for exactly that reason - not
responsible. For example, it may be that the simple fact that the commanding officer is aware of
this problem will be enough for an attribution of responsibility. And further it is plausible to
suppose that the decision makers are obliged to inform themselves on exactly these kinds of
risks. Here the context of deployment may make a difference. In the case of the military chain of
command we may be unwilling to make the commanding officer responsible for the
malfunctioning of a machine despite her awareness because the commanding officer is herself
subject to orders - still, someone else within that chain of command will then be bearer of
responsibility.

Decision makers must inform themselves about the levels of flexibility and responsiveness to the
environment of the machines and about possible malfunctioning related to this. In making their
deployment decisions at the different levels, decision makers are or should be aware of this and
are therefore responsible for the consequences of their deployment decisions – at least those that
could have been reasonably foreseen, where obviously this condition is more challenging when it
comes to autonomous robots than with simple instruments or machines.

So there is no responsibility gap insofar as the decision makers can be held responsible for all the
malfunctioning and unpredicted functioning that could have been reasonably predicted.
Admittedly, what would have been reasonable to predict is, in this case, possibly more
challenging than in more traditional decision-making; and future courts and legislators may have
a very hard time in making a solid case for malfunctioning cases.

However, some of this complication is not new and should therefore not be overstated. Take for
example decision-making chains. Even if robots or machines are not involved, attributing
responsibility within a complicated decision-making chain such as a state or company is already
a very difficult task and often when it comes to legal responsibility courts do fail due to the
sophistication of the decision-making chain. Robots may make this even more difficult but, again,
these difficulties are not new and therefore should not be overstated in the case of robots.

In addition, even when some difficulties are likely to arise, these responsibility gaps are not a
destiny. By becoming aware of the possible difficulties of future legislators and courts in
attributions of moral and legal responsibility for the malfunctioning of robots we may and should
find ways to grant that future legislators and courts will have resources that current legislators
and courts do not possess: the level of sophistication grows on both sides.

Finally, not only legislators but also designers and programmers may contribute to reduce future
responsibility gaps: we believe that ethical considerations and values should be taken into
account not at the level of use but also since the early stage of technological design. From this
perspective, known in the ethics of technology debate as value-sensitive design, human control
and responsibility maybe seen as non-functional requirements that can and should be met in the
design stage of socio-technical systems.

## Beyond the responsibility gap: a pluralist and interdisciplinary perspective on drones and responsibility

One simple reaction to the debate on military drones and responsibility would consist of taking a
reductionist perspective: insisting that drones are just (very complicated) tools, so that their use
does not detract in any way from the responsibility of the human beings who deploy them.

In the end, so the argument goes, the burden of responsibility for the use of drones in military
operations - and the consequence of this - remain on politicians and commanders who decide whether
and when to deploy drones, and are thus responsible for the (mis)use or malfunctioning of drones in
military operations, and the consequences of this (mis)use or malfunctioning. In this reductionist
perspective, the only relevant question about drones and responsibility is therefore whether and
under which conditions the use by States of (which) military drones is permissible, impermissible,
or obligatory.

We admit that drones are in an important sense just tools, machines to which no moral or
political responsibility can be assigned; and we also recognize that that of the
permissibility/impermissibility/obligatoriness of the use of drones by (democratic) States is
certainly one very important issue to be discussed. However, we also think that the deployment
of drones in military operations does pose other interesting questions of responsibility.

There are good reasons to explore these drones and responsibility issues in some depth:
* Some quantitative changes are more relevant than others: for instance, the
passage from a knife to a gun can be of more momentum than the passage from the club to the
knife. Due to the specific features of different tools, designers, produces, sellers, legislators may
have responsibilities for the wrong use of guns which they wouldn’t have for the wrong use of knives.
* Even if the analysis had to reveal that drones do not pose any significantly new
question of responsibility, we think that drones may represent a new interesting case study whose
analysis may contribute to a rethinking, a deepening, and possibly an extension of our
understanding of responsibility, technology, and warfare – or maybe just of responsibility *tout court*.

In order to map the different issues at stake, we suggest a pluralist and interdisciplinary
approach to drones and responsibility. Our approach is pluralist as in contrast with the
responsibility gap theorists, our focus is not only on individual backward-looking moral
blameworthiness for conscious and/or intended and/or rationally deliberated acts. We think that
there are other kinds of responsibility to be attributed to different individual and collective actors
in different circumstances.

We take an interdisciplinary approach, as we think that in order to map as many interesting
issues of responsibility as possible, contributions from different disciplinary perspective are
required: not only philosophy but also law and science/engineering.

## Varieties of responsibility

A first important distinction is that between moral, legal and political
responsibility. By moral responsibility we mean the responsibilities which arise by simply being
humans acting in a space with other humans. A paradigmatic example of moral responsibility is
the blameworthiness for stealing something or intentionally injuring someone. By legal and
political responsibility we mean those specific responsibilities which come into existence due to
the presence of a particular legal or a political system of rules. A paradigmatic example of legal
responsibility would be A’s liability to pay a certain sum of money to B, a liability grounded in a
valid legal contract existing between A and B. A paradigmatic example of political responsibility
would be the duty of a democratic government to give their citizen an account of their activities.

In addition, moral, political, and legal responsibility can be backward-looking or forward-
looking. Backward-looking responsibility designates the normative position of an actor in
relation to facts that have already occurred, for instance someone being subject to moral blame
(or praise) for one particular past action of them; forward-looking responsibility concerns the
normative position of an actor in relation to future actions and states of affair, for instance a
government being subject to an obligation to take preventive steps in order to avoid the
occurrence of certain accidents on a given territory.

Backward-looking responsibility (be it moral, political, or legal) comes in at least five different
kinds:

* **capacity-responsibility**: one is responsible if they are the kind of agents who
possess the prerequisites to be attributed (a certain kind) of backward-looking responsibility. A
typical example of capacity-responsibility is a person’s ability to have meaningful moral
interactions with others, that is being a sane adult able to understand the nature and meanings of
their actions and the effects of these on others.
* **accountability**: the normative position of who ought to report and explain - indeed,
to give an account - of something that has happened. Typical example of accountability are: an
individual person having the obligation to give reasons for a certain morally relevant action she
made - in order to be attributed praise or blame or simply to help other make sense of what
happened; and a democratic government being obliged to inform their citizens about their
activities and the results of these.
* **liability**: the position of who is subject to negative (moral, legal, political)
consequences for their actions and omissions. Typical examples would be moral blame and legal
punishment for individuals’ wrong actions. At least two main forms: liability for intended actions,
and liability for unintended but still culpable actions (negligent actions). It
is important to keep in mind that not all wrong actions attract liability. Standard ways to avoid
liability for wrong behaviour are:
  * **exemptions**: make
subjects in general not suitable to take part in the attribution of liability game; one typical
exemption from individual liability is lack of the relevant capacity (for instance an individual
being affected by a serious mental disorder affecting her ability to perceive the world correctly).
  * **justifications**: work by making a prima facie wrong behaviour all things considered permissible,
by pointing to the presence of some exceptional circumstances; one typical example is self-
defence as a justification for a killing.
  * **excuses**: work by making a prima facie liable actor all
things considered non-blameworthy by pointing to some particular circumstances of the action;
typical examples are non-culpable ignorance of relevant circumstances and being coerced.
* **causal responsibility**: designates the position of who has acted in such a way as to give –
possibly without intention, or even without knowledge - a substantial causal contribution to a
certain (negative) outcome. A typical example would be that of a scientist unknowingly
contributing to the research which eventually lead to the production and use of a weapon of mass
destruction.
* **role**: designates the position of who is liable for untoward events because
of occupying a certain position, and thus independently from her having directly caused them
with her behaviour. A typical example of role-responsibility would be the liability of parents for
their children behaviour, or that of a commander for the misdeeds of the soldiers under his
command.

As for the different actors potentially involved in various issues of responsibility as raised by the
use of military drones, we think that at least the following have to be considered: democratic
states, individual military personnel (commanders and drone operators), designers and
programmers.

## Drones and responsibility: legal, philosophical and socio-technical perspectives

By considering the different actors involved, we may identify at least three set of issues of
responsibility in relation to the use of drones:
* the responsibilities of democratic States
* the responsibilities of individual actors using military drones, that is military personnel
* the responsibilities of designers and programmers of drones

By using the conceptual distinctions about responsibility and the list of different actors of the
previous section, we can provide the following provisional list of different responsibility
questions potentially raised by the use of military drones:
* (When) are states *justified* and thus non-liable in using drones for extra-territorial
killings? When (if ever) do these killings counts as self-defence?
* May the use of drones for extra-territorial killings unduly reduce states’ *accountability* for these killings?
* May the use of drones unduly reduce the states’ ability to discharge the duties deriving
from their *causal responsibility*, e.g. the duty to compensate or apologize to the innocent victims of lawful drone strikes?
* By keeping soldiers away from the battlefield are states preventing soldiers to discharge
their *responsibility as combatants*?
* Do states have a *responsibility to recognize drone operators as combatants* and to provide them with adequate medical and psychological help?
* May the use of drones make it more difficult to hold *morally* and *legally liable* individuals who directly support with their act the commission of violations of the laws
of war, for instance by providing soldiers with a new kind of excuse – reliance on the
machine?
* May the use of drones make it more difficult to legitimately attribute *role-responsibilities*
such as the “superior” or “command” legal responsibility for war crimes?
* Is it *in principle* morally wrong to delegate (part of) the decision about killing to artificial
agents that are not responsible as they *lack the capacity to respond to moral reasons* in
the same way as humans do? Or, is delegation of these activities OK insofar as machine
are as reliable as humans in performing certain tasks?
* Which moral responsibilities derive from *human capacity* for morally autonomous
action? How should these human responsibilities be preserved by *design* in the use of
military drones?
* How can drones be *designed as to be “accountable”* (i.e. their behaviour being transparent) to human operators?
* What are the *responsibilities of drone designers towards society*? How should designers
and programmers make sure that the transition to civilian uses of drones does not bring
unwanted ethical consequences?

#### States responsibilities

the philosopher Asa Kasher has argued that democratic States have at least two kinds of
forward-looking responsibility: they have a political duty to provide its citizens with protection
of their life, wellbeing and liberty (the Duty of Self Defense), but they also have a moral duty to
respect human dignity in every activity (the principle of Human Dignity Protection). Kasher
claims that the principle of Human Dignity Protection poses a burden on democratic states to
observe strict regulations and clear limitations in the use of armed drones for targeted killings
outside their territory. In particular, the use of lethal force by the State must be limited to cases
where there is clear evidence that the targeted person represents an imminent threat to the life of
the State’s citizens; in addition, these operations must be regulated by the traditional war ethics
principles of proportionality and necessity. However, according to Kasher, if all these conditions
are met and all the constraints observed, the Duty of Self Defence makes the use of military force
by the State for targeted killings not only morally permissible, but also morally obligatory.
Kasher relies on the traditional justification of self-defence to claim that some killings are
permissible and not morally blameworthy, and their perpetrators not legally liable for them.

Some objections to Kasher’s view can be raised from an ethical as well as from a legal
perspective. Firstly, as remarked in this volume by Bernard Koch, Kasher’s interpretation of the
principle of self-defence can be challenged from a moral point of view. On the one hand, the
principle of self-defence is commonly thought to justify a killing only if at the moment of the
killing the victim was posing an immediate lethal threat to the killer; yet it seems that many if not
all of those whom Kasher would consider the legitimate target of a State’s lethal drone attack
cannot be seen as an immediate threat to the life of the state’s citizens in the paradigmatic sense
typical of the doctrine of self-defence. Whilst in the standard cases of justified killings in self-
defence the victim is killed while she is unlawfully attacking the killer, targets of drone attacks
are normally individuals living outside the State, who at the time of the drone attack are thought
to be planning some terrorist attack on the basis of intelligence investigations. On the other hand,
for this same reason, in order to be morally justified, a killing in self-defence should not be
planned and programmed but should rather be the outcome of a last resort attempt of
incapacitating a threatening aggressor; in this respect, targeted killings by drones seem
particularly problematic because armed drones - like traditional missiles and unlike guns - are
designed not to capture or incapacitate, but only to kill.

Moreover, permissive positions on the use of drones for extraterritorial targeted killings like
Kasher’s can be challenged also from the perspective of international law. Kasher’s reliance on
a permissive interpretation of the doctrine of self-defence as a justification for targeted killings
by drones seem to leave too much space for the States to elude other their responsibilities.

Firstly, whereas self-defence may sometimes make some killings all things considered justified,
self-defence is not a licence to kill, that is a full exemption from responsibility. This means, in
particular, that those who kill in self-defence are still accountable for what they did, that is they
have to give a full report of their behaviour, not least in order to give evidence that the conditions
for lethal self-defence were met in that specific case. But, Chantal Meloni claims in her chapter,
as current lethal drone attacks are made on the basis of undisclosed intelligence information and
reports, States are de facto not accountable for their extraterritorial killings by drones; in the
absence of a reliable account of facts, we run the serious risks of leaving serious war crimes
unpunished. And in any case, the use of drones for targeted killings may create “a major
accountability vacuum”.

Secondly, and relatedly, a justification does eliminate blameworthiness but it does not eliminate
causal responsibility and the related duty to apologize to and possibly compensate the victims of
one’s actions. However, when States are not held accountable for their operation and they don’t
act according to their duty to investigate on the effects of their operations, there may be no
official way to ascertain the number and identity of victims - be they the intended targets of the
attack or the unintended side-effect of it, i.e. innocent civilians; so that, even assuming that both
the intended and the unintended killings were justified - the former by the doctrine of self-
defence, the latter by the doctrine of necessity and proportionality - there would be no way for
the State to discharge their duty to apologize and maybe compensate those victims.

Finally, the States also have a duty to protect their soldiers lives and health. Indeed, one common
point of discussion in the debate on the permissibility of the use of military drones is whether it
is alright for a state to “kill by remote control”. Supporters of drones insist that
drones help the States fulfil their duty to keep soldiers as removed as possible from the dangers
of the battlefield and more generally, provided certain requirement are met, to arguably improve
the overall well-being of people. Critics observe that the remoteness from
the battlefield and the related safety for military personnel allowed for by military drones may
bel ethically problematic. It may create an unjustifiable asymmetry in war between actors that
possess drones and actors who do not and/or a dangerous lightheartedness in the military personnel
(the ‘playstation mindset’) or a lowering of the threshold for waging war by states. Without denying
the obvious moral advantages stressed by drone supporters, Koch goes as far as to
claim that this march towards more and more safety for soldiers may also be seen as intrinsically
wrong in some respect. According to Koch’s “existentialist” perspective, the state’s
responsibility to grant their citizens and soldiers as much safety and well-being as possible may
be in conflict with the individuals’ responsibility towards themselves, that is people’s duty to be
faithful to their moral identity; and for soldiers, Koch argues, this identity critically depends on
their putting their lives at risk.

As distant as they may be in their moral and political evaluations, what both parties in this debate
seem to agree on is that drone war is not dangerous for drone operators. However, as extensively
argued in his chapter by Jesse Kirkpatrick, this may not be the case. Drone combat may
actually share important features with traditional combat, and drones operators may be thus exposed
at least to a similar psychological harm than traditional soldiers. If this is the case, states
may also have a duty to recognize drone operators as professional combatants not just
lighthearted computer geeks; and to make sure that drone operators have all the kinds of support
they may need (e.g. adequate training, medical and psychological help, etc.).

#### Individual responsibilities of miltary personnel and politicians

Meloni highlights some difficulties in the ascertainment of legal responsibility before domestic
courts for targeted killings by drones and their collateral effects. However, things may be different
with the use of drones in more conventional war operations.

Saxon analyses two
kinds of legal responsibility: the liability of individuals who directly support with their act the
commission of violations of the laws of war by the use of drones; and the “superior” or
“command” responsibility “deriving from the failure of military and civilian superiors to perform
their duty to prevent their subordinates from committing such crimes” (a typical example of role-
responsibility). Saxon admits that the presence on the battlefield of complex automated or semi-
automated weapon systems like drones may complicate the task of commander to supervise and
make sure that no mistakes are done in war operations; and that an “accountability gap” would
exist in circumstances in which a drone makes a ‘mistake’ leading to an unlawful attack, while at
the same time the commander could not in any way anticipate that this could happen. However,
Saxon claims, this *“would not be qualitatively different from other ‘accountability gaps’ found in
modern warfare and tolerated by international law”*. Moreover, the use of military drones may
sometimes even enhance the capacity to trace legal responsibilities due to the ability of digital
technology to record and store information about its activity.

Legal considerations apart, one may still wonder whether the introduction of a (semi)automated
machine mediating warfare operations may sometimes offer individual military personnel a valid
excuse from moral responsibility in relation to some unlawful behaviour. Since World War II
and the following Nuremberg Trials, soldiers responsible for the commission of war crimes are
not allowed any more to defend themselves by appealing to the fact that they followed orders.
However, Alex Leveringhaus wonders whether in the next future drone operators will be able to
legitimately appeal to the fact that they followed ‘what automated mechanisms told’ them. For
instance, an unlawful killing of innocent civilians may be caused by the operator S relying on
what it turned out to be a wrong information provided by the automatic system of observation of
the drone. In such a situation it seems that the operator S may point to his ignorance (or, in
Leveringhaus’ words: lack of moral awareness) as a possible excuse for his misbehaviour; in
fact, he may successfully argues that he simply didn’t know that there were innocent civilians in
the area which he targeted. However, Leveringhaus argues, whereas operator S’s behaviour may
be judged as non-culpable by focussing on the circumstances at the time of the deployment of
drone, S can still can be judged negligent for deploying and relying on a technology which put
innocent lives at risks (under certain circumstances); or even for simply not taking *“the adequate
steps to attain the morally relevant facts that enabled him to assess the risks arising from
automation”*.

In his chapter on the relationship between drones and moral responsibility, Nikil Mukerji, while
dissenting on Sparrow’s thesis that the introduction of autonomous robot would lead to a
responsibility gap, agrees that there should be limits to the autonomy of military action granted
to drones. his main argument is that
machines that are not responsible in the capacity sense – that is, are not responsive to the variety
of reasons for action typically responded to by human agents – should not be granted the
possibility to take morally relevant decisions – in this case, killing – no matter how sophisticated
their capacity for perception, calculus, etc. may be.

However, Asa Kasher disagrees with a similar *a priori* moral
approach and claims that military drones may sometimes be allowed to take life-and-death
decisions, provided they passed a variation on Alan Turing’s famous Imitation game.

Finally, Michael Funk and colleagues reflect on the relationship between human autonomy and
the development of (military) autonomous technology from a broader philosophical and
anthropological perspective. Their normative conclusion is that, in contrast with their tendency to
mirror themselves in their technological creations, humans should remain aware of the
specificity of their autonomy of action, and not giving up their political responsibility to regulate
the use of these technologies in the light of ambitious moral ideals.

#### Designers' and programmers' responsibilities

Leveringhaus claims that not only drone pilots but also drone programmers may be legitimately
be burdened with a high demand for moral awareness; because of the high stakes involved in the
use of automated weapon system and because of their being in the position of taking reflected
choices far away from the heat of the battlefield.

This approach is consistent with the so called value-sensitive design approach in engineering.
Scientists and engineers are very often causally responsible for serious political and
military catastrophes. It is debatable whether and under which conditions
scientists can be held morally - or even legally - responsible for the unlawful use of the
technology which they contributed to create. Arguably, if scientists and engineers were aware of
possible catastrophic misuses of these technology they may share some moral responsibility for
these misuses.

However, according to the value-sensitive Design approach (VSD), the main focus of the ethical
reflection on technology and responsibility should not be on the issue of backward-looking moral
responsibility of engineers and designers, but rather on the question about their forward-looking
moral responsibilities. In this perspective, designers and programmers should anticipate possible
ethical, political, and societal issues and risks posed by their technology; and they should try to
design technology, since the early stages, in such a way as to address these issues and to
minimize those risks. From this perspective Tjerk De Greef argues that the challenge for a
drone designer is finding a way to avoid accountability gaps. Automated machines are not
morally responsible – humans operators are. Yet designers are responsible too; they ought to
make sure that operators relying on automated systems not only receive the best available
information from the system; they should also be able to assess the reliability of this information
themselves. In order to achieve this goal, machines have to be designed and programmed in such
a way as to become, in a metaphorical sense, “accountable” partners to their operators. Operators
must be put in the position to understand the “reasoning” behind a certain information or
suggestion provided by the machine, and reaching their final decision by actively interacting
with the machine.

While sharing a Value-Sensitive Design approach to military drones, Aimee Van Wynsberghe
and Michael Nagenborg broaden its application beyond the limits of war operations. If one
considers the history of the technology deployed in warfare in the past decades - so they argue -
and in particular how technology designed for military purposes has become widely used in
civilian contexts - internet is just one striking example - one must conclude that there is no such a
thing as a “military technology”. But if this is the case, and drones will be used more and more in
different civilian contexts (e.g. in agriculture, in policing, in rescue and disaster management) it
is the designers’ responsibility also to manage this transition; by starting anticipating which
values are at stake in these other domains as opposed to the military domain; and by working on
how to re-desing drones in such a way as to respect and promote these other values.


